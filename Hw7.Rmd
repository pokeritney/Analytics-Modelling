---
title: "Hw7"
output: word_document
---

Question 10.1(a)
Set up file and build regression tree model first:
```{r}
rm(list = ls())
set.seed(42)
uscrime <- read.table("uscrime.txt", stringsAsFactors = FALSE, header = TRUE) #import uscrime data
library(tree)
uscrime_tree <- tree(Crime~., data = uscrime) #build a regression tree model
summary(uscrime_tree) #view the output of regression tree
```

```{r}
uscrime_tree$frame
uscrime_tree$where
```
var  n        dev      yval splits.cutleft splits.cutright
1     Po1 47 6880927.66  905.0851          <7.65           >7.65
2     Pop 23  779243.48  669.6087          <22.5           >22.5
4      LF 12  243811.00  550.5000        <0.5675         >0.5675
8  <leaf>  7   48518.86  466.8571                               
9  <leaf>  5   77757.20  667.6000                               
5  <leaf> 11  179470.73  799.5455                               
3      NW 24 3604162.50 1130.7500          <7.65           >7.65
6     Pop 10  557574.90  886.9000          <21.5           >21.5
12 <leaf>  5  146390.80 1049.2000                               
13 <leaf>  5  147771.20  724.6000                               
7     Po1 14 2027224.93 1304.9286          <9.65           >9.65
14 <leaf>  6  170828.00 1041.0000                               
15 <leaf>  8 1124984.88 1502.8750                               

Visualize regression tree:
```{r}
plot(uscrime_tree)
text(uscrime_tree)
uscrime_tree
```
Manually comute R2 to estimate model quality.
```{r}
yhat <- predict(uscrime_tree)
SSres <- sum((yhat-uscrime$Crime)^2)
SStot <- sum((uscrime$Crime-mean(uscrime$Crime))^2)
R2 <- 1-SSres/SStot
R2
```
Examining training and CV deviance for different tree sizes. Should we prune some branches?
```{r}
prune.tree(uscrime_tree)$size
prune.tree(uscrime_tree)$dev
set.seed(42)
cv.tree(uscrime_tree)$dev
```
Manually pruning a tree to only 4 leaves.
```{r}
uscrime_tree_prune <- prune.tree(uscrime_tree, best = 4)
plot(uscrime_tree_prune)
text(uscrime_tree_prune)
```

Question 10.1(b)
Determine our number of predictors and grow random trees.
We can choose n/3 = 16 predictors/3, let's pick 5.
```{r}
rm(list = ls())
uscrime <- read.table("uscrime.txt", stringsAsFactors = FALSE, header = TRUE) #import uscrime data
library(randomForest)
set.seed(42)
num_pred <- 5 #set predictors to be 5
uscrime_rf <- randomForest(Crime~., data = uscrime, mtry = num_pred, importance = TRUE, ntree = 500) #build random forest model
uscrime_rf
```
Visualize random forest model.
```{r}
plot(uscrime_rf)
```
Compute R2 again.
```{r}
yhat <- predict(uscrime_rf)
SSres <- sum((yhat-uscrime$Crime)^2)
SStot <- sum((uscrime$Crime-mean(uscrime$Crime))^2)
R2 <- 1-SSres/SStot
R2
```

Question 10.2
A logistic regression model is appropriate when estimating the probability of a person getting diabete. We can set our response as either having the diabete(1) or not having the diabete(0).
Some predictors I will use are age, gender, blood glucose measurement, exercises per week.

Question 10.3(1)
Import german dataset and build a logistic regression.
```{r}
rm(list = ls())
germancredit <- read.table("germancredit.txt", sep = " ")
head(germancredit)
```
Make response variable binary in terms of 0 and 1. originally 1 and 2. Now "0" is good and "1" is bad.
```{r}
germancredit$V21 = germancredit$V21- 1
head(germancredit)
```
Create a logistic regression model and get summary.
```{r}
germancredit_model = glm(V21~., family=binomial(link="logit"), data = germancredit)
summary(germancredit_model)
```
Rerun the model using significant predictors.
```{r}
germancredit_model2 = glm(V21~V1+V3+V4+V5+V6+V8+V9+V10+V14, family=binomial(link="logit"), data = germancredit)
summary(germancredit_model2)
```

10.3(2)
Now need to determine the threshold. Because that incorrectly
identifying a bad customer as good, is 5 times worse than incorrectly classifying a good
customer as bad. Since '1' is bad we need to find how many are incorrectly classified, and record the cost. It will look like the following:
```{r}
mycost <- function(r, pi){
  if (r==1){ #bad customer
    if (pi < threshold) {#predicted as good customer
      return(5)
  } else{
    return(0) #true negative, no cost
  }
}}
```
Now we can use a for loop to test threshold can produce lowest cost.
```{r}
pred_val <- predict.glm(germancredit_model2, data = germancredit) #get the predicted response using 2nd model
original = germancredit$V21

```

